{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating sentence representations from Skip-gram with SentEval\n",
    "\n",
    "* Dependencies:\n",
    "    * Python 3.6 with NumPy/SciPy\n",
    "    * Pytorch \n",
    "    * SentEval\n",
    "    * scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, unicode_literals\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import logging\n",
    "import sklearn\n",
    "#import data \n",
    "# data.py is part of Senteval and it is used for loading word2vec style files\n",
    "import senteval\n",
    "import torch\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from skipgram import Skipgram\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\" dot.notation access to dictionary attributes \"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "class EmbeddingExtractor:\n",
    "    \"\"\"\n",
    "    Wraps a skip-gram model and returns embeddings for words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path):        \n",
    "        self.model = Skipgram(71578, 100)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "\n",
    "    def get_embeddings(self, words):\n",
    "        \"\"\"\n",
    "        :param sentence: np array of shape [batch_size, longest_sentence] containing the unique ids of words\n",
    "        \n",
    "        :returns: [batch_size, longest_sentence, z_dim]        \n",
    "        \"\"\"\n",
    "        return self.model.embeddings(words).data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you interface with SentEval. The only think you need to change are the paths to trained models in the main block at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-31 16:31:00,606 : ***** (Probing) Transfer task : DEPTH classification *****\n",
      "2018-05-31 16:31:01,664 : Loaded 100000 train - 10000 dev - 10000 test for Depth\n",
      "2018-05-31 16:31:03,445 : Computing embeddings for train/dev/test\n",
      "2018-05-31 16:31:33,011 : Computed embeddings\n",
      "2018-05-31 16:31:33,012 : Training sklearn-LogReg with standard validation..\n",
      "2018-05-31 16:34:32,332 : [('reg:0.25', 24.04), ('reg:0.5', 23.93), ('reg:1', 23.89), ('reg:2', 23.86), ('reg:4', 23.85), ('reg:8', 23.85)]\n",
      "2018-05-31 16:34:32,334 : Validation : best param found is reg = 0.25 with score             24.04\n",
      "2018-05-31 16:34:32,354 : Evaluating...\n",
      "2018-05-31 16:35:01,270 : \n",
      "Dev acc : 24.0 Test acc : 23.4 for DEPTH classification\n",
      "\n",
      "2018-05-31 16:35:01,277 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****\n",
      "2018-05-31 16:35:02,910 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents\n",
      "2018-05-31 16:35:05,073 : Computing embeddings for train/dev/test\n",
      "2018-05-31 16:35:34,231 : Computed embeddings\n",
      "2018-05-31 16:35:34,232 : Training sklearn-LogReg with standard validation..\n",
      "2018-05-31 16:45:03,859 : [('reg:0.25', 17.86), ('reg:0.5', 17.93), ('reg:1', 17.94), ('reg:2', 17.97), ('reg:4', 17.99), ('reg:8', 18.0)]\n",
      "2018-05-31 16:45:03,878 : Validation : best param found is reg = 8 with score             18.0\n",
      "2018-05-31 16:45:03,921 : Evaluating...\n",
      "2018-05-31 16:46:40,811 : \n",
      "Dev acc : 18.0 Test acc : 18.6 for TOPCONSTITUENTS classification\n",
      "\n",
      "2018-05-31 16:46:40,863 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****\n",
      "2018-05-31 16:46:42,332 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift\n",
      "2018-05-31 16:46:44,378 : Computing embeddings for train/dev/test\n",
      "2018-05-31 16:47:12,739 : Computed embeddings\n",
      "2018-05-31 16:47:12,741 : Training sklearn-LogReg with standard validation..\n",
      "2018-05-31 16:47:37,128 : [('reg:0.25', 49.57), ('reg:0.5', 49.56), ('reg:1', 49.56), ('reg:2', 49.54), ('reg:4', 49.54), ('reg:8', 49.55)]\n",
      "2018-05-31 16:47:37,154 : Validation : best param found is reg = 0.25 with score             49.57\n",
      "2018-05-31 16:47:37,163 : Evaluating...\n",
      "2018-05-31 16:47:41,206 : \n",
      "Dev acc : 49.6 Test acc : 49.8 for BIGRAMSHIFT classification\n",
      "\n",
      "2018-05-31 16:47:41,253 : ***** (Probing) Transfer task : TENSE classification *****\n",
      "2018-05-31 16:47:42,764 : Loaded 100000 train - 10000 dev - 10000 test for Tense\n",
      "2018-05-31 16:47:44,861 : Computing embeddings for train/dev/test\n",
      "2018-05-31 16:48:14,256 : Computed embeddings\n",
      "2018-05-31 16:48:14,258 : Training sklearn-LogReg with standard validation..\n",
      "2018-05-31 16:48:44,084 : [('reg:0.25', 64.96), ('reg:0.5', 64.97), ('reg:1', 64.95), ('reg:2', 64.94), ('reg:4', 64.94), ('reg:8', 64.94)]\n",
      "2018-05-31 16:48:44,092 : Validation : best param found is reg = 0.5 with score             64.97\n",
      "2018-05-31 16:48:44,100 : Evaluating...\n",
      "2018-05-31 16:48:49,126 : \n",
      "Dev acc : 65.0 Test acc : 65.0 for TENSE classification\n",
      "\n",
      "2018-05-31 16:48:49,193 : ***** (Probing) Transfer task : SUBJNUMBER classification *****\n",
      "2018-05-31 16:48:50,838 : Loaded 100000 train - 10000 dev - 10000 test for SubjNumber\n",
      "2018-05-31 16:48:52,946 : Computing embeddings for train/dev/test\n",
      "2018-05-31 16:49:21,719 : Computed embeddings\n",
      "2018-05-31 16:49:21,721 : Training sklearn-LogReg with standard validation..\n",
      "2018-05-31 16:49:51,068 : [('reg:0.25', 65.58), ('reg:0.5', 65.56), ('reg:1', 65.54), ('reg:2', 65.54), ('reg:4', 65.53), ('reg:8', 65.54)]\n",
      "2018-05-31 16:49:51,094 : Validation : best param found is reg = 0.25 with score             65.58\n",
      "2018-05-31 16:49:51,116 : Evaluating...\n",
      "2018-05-31 16:49:56,011 : \n",
      "Dev acc : 65.6 Test acc : 64.0 for SUBJNUMBER classification\n",
      "\n",
      "2018-05-31 16:49:56,016 : ***** (Probing) Transfer task : OBJNUMBER classification *****\n",
      "2018-05-31 16:49:57,649 : Loaded 100000 train - 10000 dev - 10000 test for ObjNumber\n",
      "2018-05-31 16:49:59,718 : Computing embeddings for train/dev/test\n",
      "2018-05-31 16:50:29,677 : Computed embeddings\n",
      "2018-05-31 16:50:29,679 : Training sklearn-LogReg with standard validation..\n",
      "2018-05-31 16:50:56,273 : [('reg:0.25', 63.71), ('reg:0.5', 63.71), ('reg:1', 63.7), ('reg:2', 63.68), ('reg:4', 63.69), ('reg:8', 63.68)]\n",
      "2018-05-31 16:50:56,287 : Validation : best param found is reg = 0.25 with score             63.71\n",
      "2018-05-31 16:50:56,293 : Evaluating...\n",
      "2018-05-31 16:51:00,249 : \n",
      "Dev acc : 63.7 Test acc : 64.9 for OBJNUMBER classification\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Depth': {'ntest': 10000, 'acc': 23.44, 'devacc': 24.04, 'ndev': 10000}, 'ObjNumber': {'ntest': 10000, 'acc': 64.89, 'devacc': 63.71, 'ndev': 10000}, 'BigramShift': {'ntest': 10000, 'acc': 49.83, 'devacc': 49.57, 'ndev': 10000}, 'Tense': {'ntest': 10000, 'acc': 64.96, 'devacc': 64.97, 'ndev': 10000}, 'SubjNumber': {'ntest': 10000, 'acc': 64.0, 'devacc': 65.58, 'ndev': 10000}, 'TopConstituents': {'ntest': 10000, 'acc': 18.63, 'devacc': 18.0, 'ndev': 10000}}\n"
     ]
    }
   ],
   "source": [
    "# Set params for SentEval\n",
    "# we use logistic regression (usepytorch: False) and kfold 10\n",
    "# In this dictionary you can add extra information that you model needs for initialization\n",
    "# for example the path to a dictionary of indices, of hyper parameters\n",
    "# this dictionary is passed to the batched and the prepare fucntions\n",
    "params_senteval = {'task_path': '',\n",
    "                   'usepytorch': False,\n",
    "                   'kfold': 10,\n",
    "                   'ckpt_path': '',\n",
    "                   'tok_path': '',\n",
    "                   'extractor': None,\n",
    "                   'tks1': None}\n",
    "# made dictionary a dotdict\n",
    "params_senteval = dotdict(params_senteval)\n",
    "# this is the config for the NN classifier but we are going to use scikit-learn logistic regression with 10 kfold\n",
    "# usepytorch = False \n",
    "#params_senteval['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n",
    "#                                 'tenacity': 3, 'epoch_size': 2}\n",
    "\n",
    "\n",
    "\n",
    "def prepare(params, samples):\n",
    "    \"\"\"\n",
    "    In this example we are going to load a tensorflow model, \n",
    "    we open a dictionary with the indices of tokens and the computation graph\n",
    "    \"\"\"\n",
    "    params.extractor = EmbeddingExtractor(model_path=params.ckpt_path)\n",
    "    # load tokenizer from training\n",
    "    params.tks1 = pickle.load(open(params.tok_path, 'rb'))\n",
    "    return\n",
    "\n",
    "def batcher(params, batch):\n",
    "    \"\"\"\n",
    "    At this point batch is a python list containing sentences. Each sentence is a list of tokens (each token a string).\n",
    "    The code below will take care of converting this to unique ids that EmbedAlign can understand.\n",
    "    \n",
    "    This function should return a single vector representation per sentence in the batch.\n",
    "    In this example we use the average of word embeddings (as predicted by EmbedAlign) as a sentence representation.\n",
    "    \n",
    "    In this method you can do mini-batching or you can process sentences 1 at a time (batches of size 1).\n",
    "    We choose to do it 1 sentence at a time to avoid having to deal with masking. \n",
    "    \n",
    "    This should not be too slow, and it also saves memory.\n",
    "    \"\"\"\n",
    "    # if a sentence is empty dot is set to be the only token\n",
    "    # you can change it into NULL dependening in your model\n",
    "    batch = [sent if sent != [] else ['.'] for sent in batch]\n",
    "    embeddings = []\n",
    "    for sent in batch:\n",
    "        # Here is where dgm4nlp converts strings to unique ids respecting the vocabulary\n",
    "        # of the pre-trained EmbedAlign model\n",
    "        # from tokens to ids, position 0 is English\n",
    "        #x1 = params.tks1[0].to_sequences([(' '.join(sent))])\n",
    "        x1 = torch.tensor([params.tks1.get(word, params.tks1['<unk>']) for word in sent], dtype=torch.long)\n",
    "        \n",
    "        # extract word embeddings in context for a sentence\n",
    "        # [1, sentence_length, z_dim]\n",
    "        z_batch1 = params.extractor.get_embeddings(x1)\n",
    "        # sentence vector is the mean of word embeddings in context\n",
    "        # [1, z_dim]\n",
    "        sent_vec = np.mean(z_batch1, axis=0)\n",
    "        # check if there is any NaN in vector (they appear sometimes when there's padding)\n",
    "        if np.isnan(sent_vec.sum()):\n",
    "            sent_vec = np.nan_to_num(sent_vec)        \n",
    "        embeddings.append(sent_vec)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Set up logger\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # define paths\n",
    "    # path to senteval data\n",
    "    # note senteval adds downstream into the path\n",
    "    params_senteval.task_path = '/home/daniel/SentEval/data/'\n",
    "    # path to computation graph\n",
    "    # we use best model on validation AER\n",
    "    # TODO: you have to point to valid paths! Use the pre-trained model linked from the top of this notebook.\n",
    "    params_senteval.ckpt_path = 'models/71576V_100d_5w_Skipgram.pt'\n",
    "    # path to tokenizer with ids of trained Europarl data\n",
    "    # out dictionary id depends on dill for pickle\n",
    "    params_senteval.tok_path = 'models/word2idx.p'\n",
    "    # we use 10 fold cross validation\n",
    "    params_senteval.kfold = 10\n",
    "    se = senteval.engine.SE(params_senteval, batcher, prepare)\n",
    "    \n",
    "    # here you define the NLP taks that your embedding model is going to be evaluated\n",
    "    # in (https://arxiv.org/abs/1802.05883) we use the following :\n",
    "    # SICKRelatedness (Sick-R) needs torch cuda to work (even when using logistic regression), \n",
    "    # but STS14 (semantic textual similarity) is a similar type of semantic task\n",
    "    #transfer_tasks = ['MR', 'CR', 'SUBJ', 'MPQA', 'TREC', 'SST2', 'SST5', 'SICKEntailment',\n",
    "    #                 'STS14', 'MRPC']\n",
    "    transfer_tasks = ['Depth', 'TopConstituents','BigramShift', 'Tense',\n",
    "'SubjNumber', 'ObjNumber']\n",
    "    # senteval prints the results and returns a dictionary with the scores\n",
    "    results = se.eval(transfer_tasks)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
