{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loading import Embeddings, load_sim_dataset, load_analogy_dataset\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Word Representations\n",
    "\n",
    "## Word similarity\n",
    "\n",
    "We will examine the use of word embeddings as representations for the meaning of words. In particular, we will use pretrained word embeddings as obtained in [1]: Dependency based embeddings and bag-of-words embeddings with k = 2 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word embeddings\n",
    "bow2embeddings = Embeddings(\"data/bow2.words\")\n",
    "bow5embeddings = Embeddings(\"data/bow5.words\")\n",
    "depembeddings = Embeddings(\"data/deps.words\")\n",
    "embedding_dict = {'BOW2': bow2embeddings, 'BOW5': bow5embeddings, 'Dependency': depembeddings}\n",
    "\n",
    "# Load similarity datasets\n",
    "simlex_pairs, simlex_scores = load_sim_dataset(\"data/SimLex-999.txt\", score_col=3, skip=1)\n",
    "men_pairs, men_scores = load_sim_dataset(\"data/MEN_dataset_natural_form_full\", score_col=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative results\n",
    "We can evaluate the word embeddings quantitatively by getting a list of words similar to a given one when using each set of word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW2\n",
      "\t batman : ['superman', 'superboy', 'aquaman', 'catwoman', 'batgirl']\n",
      "\t hogwarts : ['evernight', 'sunnydale', 'garderobe', 'blandings', 'collinwood']\n",
      "\t turing : ['non-deterministic', 'finite-state', 'nondeterministic', 'bchi', 'primality']\n",
      "\t florida : ['fla', 'alabama', 'gainesville', 'tallahassee', 'texas']\n",
      "\t object-oriented : ['aspect-oriented', 'event-driven', 'objective-c', 'dataflow', '4gl']\n",
      "\t dancing : ['singing', 'dance', 'dances', 'breakdancing', 'clowning']\n",
      "BOW5\n",
      "\t batman : ['superman', 'catwoman', 'nightwing', 'spider-man', 'superboy']\n",
      "\t hogwarts : ['dumbledore', 'dementors', 'hagrid', 'snape', 'voldemort']\n",
      "\t turing : ['non-deterministic', 'nondeterministic', 'deterministic', 'finite-state', 'reducibility']\n",
      "\t florida : ['jacksonville', 'tallahassee', 'miami', 'gainesville', 'sarasota']\n",
      "\t object-oriented : ['aspect-oriented', 'event-driven', 'smalltalk', 'prolog', 'domain-specific']\n",
      "\t dancing : ['singing', 'dance', 'dances', 'dancers', 'danced']\n",
      "Dependency\n",
      "\t batman : ['superman', 'superboy', 'supergirl', 'catwoman', 'aquaman']\n",
      "\t hogwarts : ['sunnydale', 'collinwood', 'calarts', 'greendale', 'millfield']\n",
      "\t turing : ['pauling', 'hotelling', 'heyting', 'lessing', 'hamming']\n",
      "\t florida : ['texas', 'louisiana', 'georgia', 'california', 'carolina']\n",
      "\t object-oriented : ['event-driven', 'domain-specific', 'rule-based', 'platform-independent', 'data-driven']\n",
      "\t dancing : ['singing', 'rapping', 'breakdancing', 'miming', 'busking']\n"
     ]
    }
   ],
   "source": [
    "test_words = ['batman', 'hogwarts', 'turing', 'florida', 'object-oriented', 'dancing']\n",
    "for emb in embedding_dict:\n",
    "    print(emb)\n",
    "    for word in test_words:\n",
    "        print('\\t', word, ':', embedding_dict[emb].top_similar(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Pearson   Spearman\n",
      "BOW2\n",
      "    SimLEX  0.4285    0.4141 \n",
      "       MEN  0.6777    0.6999 \n",
      "BOW5\n",
      "    SimLEX  0.3756    0.3674 \n",
      "       MEN  0.7082    0.7232 \n",
      "Dependency\n",
      "    SimLEX  0.4619    0.4456 \n",
      "       MEN  0.5974    0.6178 \n"
     ]
    }
   ],
   "source": [
    "def score_pairs(pairs, scores, embs, label):\n",
    "    gold_scores = []\n",
    "    sim_scores = []\n",
    "    for i, (word1, word2) in enumerate(pairs):\n",
    "        if word1 in embs.word2idx and word2 in embs.word2idx:\n",
    "            # Get score from gold standard\n",
    "            gold_scores.append(scores[i])\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            sim_scores.append(embs.similarity(word1, word2))\n",
    "            \n",
    "    # Get pearson and spearman correlation\n",
    "    pearson_coeff = pearsonr(gold_scores, sim_scores)[0]\n",
    "    spearman_coeff = spearmanr(gold_scores, sim_scores).correlation\n",
    "    \n",
    "    print('{:>10} {:^8.4f}  {:^8.4f}'.format(label, pearson_coeff, spearman_coeff))\n",
    "\n",
    "print('{:10} {:^8s}  {:^8s}'.format('', 'Pearson', 'Spearman'))\n",
    "for emb_name in embedding_dict:\n",
    "    print('{:s}'.format(emb_name))    \n",
    "    score_pairs(simlex_pairs, simlex_scores, embedding_dict[emb_name], 'SimLEX')    \n",
    "    score_pairs(men_pairs, men_scores, embedding_dict[emb_name], 'MEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogy\n",
    "\n",
    "We now examine the usage of word embeddings to solve questions of the form \"**a** is to **b** as **c** is to **?**\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['queen', 'princess', 'berengaria', 'monarch', 'king-emperor']\n"
     ]
    }
   ],
   "source": [
    "answers = bow5embeddings.analogy('man', 'king', 'woman')\n",
    "print(answers[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW2\n",
      "1000/19544 acc: 71 mrr: 78\n",
      "2000/19544 acc: 68 mrr: 76\n",
      "3000/19544 acc: 67 mrr: 75\n",
      "4000/19544 acc: 67 mrr: 75\n",
      "5000/19544 acc: 65 mrr: 73\n",
      "6000/19544 acc: 57 mrr: 65\n",
      "7000/19544 acc: 54 mrr: 62\n",
      "8000/19544 acc: 52 mrr: 60\n",
      "9000/19544 acc: 52 mrr: 61\n",
      "10000/19544 acc: 49 mrr: 58\n",
      "11000/19544 acc: 50 mrr: 58\n",
      "12000/19544 acc: 53 mrr: 61\n",
      "13000/19544 acc: 54 mrr: 62\n",
      "14000/19544 acc: 54 mrr: 62\n",
      "15000/19544 acc: 55 mrr: 64\n",
      "16000/19544 acc: 56 mrr: 64\n",
      "17000/19544 acc: 56 mrr: 64\n",
      "18000/19544 acc: 57 mrr: 65\n",
      "19000/19544 acc: 58 mrr: 66\n",
      "acc: 58 mrr: 66\n",
      "BOW5\n",
      "1000/19544 acc: 82 mrr: 87\n",
      "2000/19544 acc: 77 mrr: 84\n",
      "3000/19544 acc: 75 mrr: 83\n",
      "4000/19544 acc: 73 mrr: 82\n",
      "5000/19544 acc: 72 mrr: 81\n",
      "6000/19544 acc: 64 mrr: 73\n",
      "7000/19544 acc: 62 mrr: 70\n",
      "8000/19544 acc: 60 mrr: 69\n",
      "9000/19544 acc: 60 mrr: 69\n",
      "10000/19544 acc: 56 mrr: 65\n",
      "11000/19544 acc: 56 mrr: 65\n",
      "12000/19544 acc: 58 mrr: 67\n",
      "13000/19544 acc: 58 mrr: 67\n",
      "14000/19544 acc: 59 mrr: 68\n",
      "15000/19544 acc: 60 mrr: 69\n",
      "16000/19544 acc: 61 mrr: 69\n",
      "17000/19544 acc: 60 mrr: 69\n",
      "18000/19544 acc: 61 mrr: 69\n",
      "19000/19544 acc: 61 mrr: 70\n",
      "acc: 61 mrr: 70\n",
      "Dependency\n",
      "1000/19544 acc: 25 mrr: 37\n",
      "2000/19544 acc: 18 mrr: 29\n",
      "3000/19544 acc: 15 mrr: 26\n",
      "4000/19544 acc: 13 mrr: 24\n",
      "5000/19544 acc: 13 mrr: 23\n",
      "6000/19544 acc: 12 mrr: 21\n",
      "7000/19544 acc: 12 mrr: 21\n",
      "8000/19544 acc: 12 mrr: 21\n",
      "9000/19544 acc: 16 mrr: 24\n",
      "10000/19544 acc: 15 mrr: 24\n",
      "11000/19544 acc: 19 mrr: 27\n",
      "12000/19544 acc: 24 mrr: 32\n",
      "13000/19544 acc: 26 mrr: 34\n",
      "14000/19544 acc: 29 mrr: 37\n",
      "15000/19544 acc: 28 mrr: 36\n",
      "16000/19544 acc: 28 mrr: 36\n",
      "17000/19544 acc: 30 mrr: 38\n",
      "18000/19544 acc: 32 mrr: 40\n",
      "19000/19544 acc: 35 mrr: 43\n",
      "acc: 36 mrr: 44\n"
     ]
    }
   ],
   "source": [
    "analogies = load_analogy_dataset('data/questions-words.txt')\n",
    "\n",
    "def get_analogy_stats(embs, analogies):\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    acc_rank = 0\n",
    "    for i, (a, b, c, d) in enumerate(analogies):\n",
    "        # Check first if we have embeddings for all words\n",
    "        if all (word in embs.word2idx for word in (a, b, c)):\n",
    "            count += 1\n",
    "            # Get analogy results\n",
    "            results = embs.analogy(a, b, c)\n",
    "\n",
    "            # Update accuracy\n",
    "            if results[0] == d:\n",
    "                correct += 1\n",
    "\n",
    "            # Update MRR\n",
    "            try:            \n",
    "                rank = results.index(d) + 1            \n",
    "                acc_rank += 1/rank          \n",
    "            except ValueError:\n",
    "                # If word was not ranked, give rank score = 0\n",
    "                continue\n",
    "        \n",
    "        # Print progress and stats so far\n",
    "        if (count%1000) == 0:\n",
    "            accuracy = int(100 * correct/count)\n",
    "            mrr = int(100 * acc_rank/count)\n",
    "            print('{:d}/{:d} acc: {:d} mrr: {:d}'.format(count, len(analogies), accuracy, mrr))\n",
    "\n",
    "    accuracy = int(100 * correct/count)\n",
    "    mrr = int(100 * acc_rank/count)\n",
    "    print('acc: {:d} mrr: {:d}'.format(accuracy, mrr))\n",
    "\n",
    "for emb_name in embedding_dict:\n",
    "    print('{:s}'.format(emb_name))\n",
    "    get_analogy_stats(embedding_dict[emb_name], analogies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] Levy, O., & Goldberg, Y. (2014). Dependency-based word embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (Vol. 2, pp. 302-308)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
